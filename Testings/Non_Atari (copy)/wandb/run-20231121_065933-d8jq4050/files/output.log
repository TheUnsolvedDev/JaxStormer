***********************************
[31mTraining started on ENV [34mCartPole-v1
[37m***********************************
[37m[3m                             Q_model_small Summary                              
â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m path    [22mâ”ƒ[1m module        [22mâ”ƒ[1m inputs      [22mâ”ƒ[1m outputs     [22mâ”ƒ[1m params                 [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         â”‚ Q_model_small â”‚ float32[4]  â”‚ float32[2]  â”‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense_0 â”‚ Dense         â”‚ float32[4]  â”‚ float32[16] â”‚ bias: float32[16]      â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ kernel: float32[4,16]  â”‚
â”‚         â”‚               â”‚             â”‚             â”‚                        â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ [1m80 (320 B)[22m             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense_1 â”‚ Dense         â”‚ float32[16] â”‚ float32[16] â”‚ bias: float32[16]      â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ kernel: float32[16,16] â”‚
â”‚         â”‚               â”‚             â”‚             â”‚                        â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ [1m272 (1.1 KB)[22m           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dense_2 â”‚ Dense         â”‚ float32[16] â”‚ float32[2]  â”‚ bias: float32[2]       â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ kernel: float32[16,2]  â”‚
â”‚         â”‚               â”‚             â”‚             â”‚                        â”‚
â”‚         â”‚               â”‚             â”‚             â”‚ [1m34 (136 B)[22m             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚[1m         [22mâ”‚[1m               [22mâ”‚[1m             [22mâ”‚[1m       Total [22mâ”‚[1m 386 (1.5 KB)           [22mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1m                                                                                
[1m                         Total Parameters: 386 (1.5 KB)                         
  2%|â–ˆâ–ˆâ–                                                                                                                    | 2010/100001 [00:02<02:11, 744.93it/s]
Traceback (most recent call last):
  File "/home/shuvrajeet/Documents/GitHub/JaxStormer/ReinforcementLearning/Non_Atari (copy)/main.py", line 133, in <module>
    main()
  File "/home/shuvrajeet/Documents/GitHub/JaxStormer/ReinforcementLearning/Non_Atari (copy)/main.py", line 129, in main
    agent.train()
  File "/home/shuvrajeet/Documents/GitHub/JaxStormer/ReinforcementLearning/Non_Atari (copy)/dqn.py", line 141, in train
    states, actions, rewards, next_states, dones = self.replay_buffer.get_batch(
                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shuvrajeet/Documents/GitHub/JaxStormer/ReinforcementLearning/Non_Atari (copy)/memory.py", line 23, in get_batch
    states = jnp.array(states).reshape((NUM_ENVS*len(indices), -1))
                                        ^^^^^^^^
NameError: name 'NUM_ENVS' is not defined